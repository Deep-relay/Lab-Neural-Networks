{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## This Notebook is used to understand the basic working of Neurons and implement simple logic Gates using a single Neuron"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets first define some activation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets first define some activation functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.11920292, 0.26894142, 0.5       , 0.73105858, 0.88079708])"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def sigmoid(x):\n",
    "        \"\"\"\n",
    "        Sigmoid activation function: f(x) = 1 / (1 + e^(-x))\n",
    "        \n",
    "        - Outputs values between 0 and 1\n",
    "        - Smooth, differentiable curve\n",
    "        - Good for binary classification problems\n",
    "        \"\"\"\n",
    "        # Clip x to prevent overflow in exponential\n",
    "        x = np.clip(x, -500, 500)\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "    \n",
    "def tanh(x):\n",
    "    return np.tanh(x)\n",
    "sigmoid(np.array([-2, -1, 0, 1, 2]))  # Example usage of sigmoid function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic Neuron Forward Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward( inputs,weights, bias):\n",
    "        \n",
    "    # Convert inputs to numpy array for easier computation\n",
    "    inputs = np.array(inputs)\n",
    "\n",
    "    # Step 1 & 2: Weighted sum (dot product of inputs and weights)\n",
    "    # This is like: w1*x1 + w2*x2 + w3*x3 + ... + wn*xn\n",
    "    weighted_sum = np.dot(inputs, weights)\n",
    "\n",
    "    # Step 3: Add bias\n",
    "    # Bias allows the neuron to fire even when inputs are zero\n",
    "    z = weighted_sum + self.bias\n",
    "\n",
    "    # Step 4: Apply activation function\n",
    "    output = sigmoid(z)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets put these into a Neuron Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicNeuron:\n",
    "    \"\"\"\n",
    "    A basic artificial neuron implementation that mimics biological neurons.\n",
    "    \n",
    "    The neuron receives inputs, applies weights, adds bias, and produces an output\n",
    "    through an activation function - just like neurons in our brain!\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_inputs, activation_function='sigmoid'):\n",
    "        \"\"\"\n",
    "        Initialize the neuron with random weights and bias.\n",
    "        \n",
    "        Args:\n",
    "            num_inputs: Number of input connections to this neuron\n",
    "            activation_function: Type of activation function ('sigmoid', 'relu', 'tanh')\n",
    "        \"\"\"\n",
    "        # Initialize weights randomly between -1 and 1\n",
    "        # Each input gets its own weight - this determines how important each input is\n",
    "        self.weights = np.random.uniform(-1, 1, num_inputs)\n",
    "        \n",
    "        # Initialize bias - this shifts the activation function left or right\n",
    "        # Bias helps the neuron fire even when inputs are small\n",
    "        self.bias = np.random.uniform(-1, 1)\n",
    "        \n",
    "        # Store the activation function type\n",
    "        self.activation_function = activation_function\n",
    "        \n",
    "        # Store the number of inputs for validation\n",
    "        self.num_inputs = num_inputs\n",
    "        \n",
    "        print(f\"Neuron created with {num_inputs} inputs\")\n",
    "        print(f\"Initial weights: {self.weights}\")\n",
    "        print(f\"Initial bias: {self.bias}\")\n",
    "        print(f\"Activation function: {activation_function}\")\n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        Sigmoid activation function: f(x) = 1 / (1 + e^(-x))\n",
    "        \n",
    "        - Outputs values between 0 and 1\n",
    "        - Smooth, differentiable curve\n",
    "        - Good for binary classification problems\n",
    "        \"\"\"\n",
    "        # Clip x to prevent overflow in exponential\n",
    "        x = np.clip(x, -500, 500)\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "    \n",
    "    def relu(self, x):\n",
    "        return np.maximum(0, x)\n",
    "    \n",
    "    def tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "    \n",
    "    def forward(self, inputs):\n",
    "        \n",
    "        # Convert inputs to numpy array for easier computation\n",
    "        inputs = np.array(inputs)\n",
    "        \n",
    "        # Validate input size\n",
    "        if len(inputs) != self.num_inputs:\n",
    "            raise ValueError(f\"Expected {self.num_inputs} inputs, got {len(inputs)}\")\n",
    "        \n",
    "        # Step 1 & 2: Weighted sum (dot product of inputs and weights)\n",
    "        # This is like: w1*x1 + w2*x2 + w3*x3 + ... + wn*xn\n",
    "        weighted_sum = np.dot(inputs, self.weights)\n",
    "        \n",
    "        # Step 3: Add bias\n",
    "        # Bias allows the neuron to fire even when inputs are zero\n",
    "        z = weighted_sum + self.bias\n",
    "        \n",
    "        # Step 4: Apply activation function\n",
    "        if self.activation_function == 'sigmoid':\n",
    "            output = self.sigmoid(z)\n",
    "        elif self.activation_function == 'relu':\n",
    "            output = self.relu(z)\n",
    "        elif self.activation_function == 'tanh':\n",
    "            output = self.tanh(z)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown activation function: {self.activation_function}\")\n",
    "        \n",
    "        # Store intermediate values for educational purposes\n",
    "        self.last_inputs = inputs\n",
    "        self.last_weighted_sum = weighted_sum\n",
    "        self.last_z = z\n",
    "        self.last_output = output\n",
    "        \n",
    "        return output\n",
    "    \n",
    "    def update_weights(self, new_weights, new_bias=None):\n",
    "        if len(new_weights) != self.num_inputs:\n",
    "            raise ValueError(f\"Expected {self.num_inputs} weights, got {len(new_weights)}\")\n",
    "        \n",
    "        self.weights = np.array(new_weights)\n",
    "        \n",
    "        if new_bias is not None:\n",
    "            self.bias = new_bias\n",
    "        \n",
    "        print(f\"Weights updated to: {self.weights}\")\n",
    "        print(f\"Bias updated to: {self.bias}\")\n",
    "    \n",
    "    def get_details(self):\n",
    "        if hasattr(self, 'last_inputs'):\n",
    "            print(\"\\n--- Neuron Computation Details ---\")\n",
    "            print(f\"Inputs: {self.last_inputs}\")\n",
    "            print(f\"Weights: {self.weights}\")\n",
    "            print(f\"Weighted sum: {self.last_weighted_sum:.4f}\")\n",
    "            print(f\"Bias: {self.bias:.4f}\")\n",
    "            print(f\"z (weighted sum + bias): {self.last_z:.4f}\")\n",
    "            print(f\"Final output: {self.last_output:.4f}\")\n",
    "        else:\n",
    "            print(\"No computation has been performed yet!\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now Lets use this class and test it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "1. Creating a neuron with 2 inputs (sigmoid activation)\n",
      "Neuron created with 2 inputs\n",
      "Initial weights: [0.93136561 0.74875981]\n",
      "Initial bias: 0.46940278228883225\n",
      "Activation function: sigmoid\n"
     ]
    }
   ],
   "source": [
    "# Example 1: Simple 2-input neuron\n",
    "print(\"\\n1. Creating a neuron with 2 inputs (sigmoid activation)\")\n",
    "neuron1 = BasicNeuron(num_inputs=2, activation_function='sigmoid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets Test it by adding some inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input: [0.5, 0.3]\n",
      "Output: 0.7613\n",
      "\n",
      "--- Neuron Computation Details ---\n",
      "Inputs: [0.5 0.3]\n",
      "Weights: [0.93136561 0.74875981]\n",
      "Weighted sum: 0.6903\n",
      "Bias: 0.4694\n",
      "z (weighted sum + bias): 1.1597\n",
      "Final output: 0.7613\n"
     ]
    }
   ],
   "source": [
    "# Test with some inputs\n",
    "test_inputs = [0.5, 0.3]\n",
    "output1 = neuron1.forward(test_inputs)\n",
    "print(f\"\\nInput: {test_inputs}\")\n",
    "print(f\"Output: {output1:.4f}\")\n",
    "neuron1.get_details()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lets test it for different activation function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "2. Same neuron architecture but with ReLU activation\n",
      "Neuron created with 2 inputs\n",
      "Initial weights: [ 0.97371038 -0.24269971]\n",
      "Initial bias: -0.9816084523321802\n",
      "Activation function: relu\n",
      "\n",
      "Weights updated to: [0.93136561 0.74875981]\n",
      "Bias updated to: 0.46940278228883225\n",
      "\n",
      "Input: [0.5, 0.3]\n",
      "Output: 1.1597\n",
      "\n",
      "--- Neuron Computation Details ---\n",
      "Inputs: [0.5 0.3]\n",
      "Weights: [0.93136561 0.74875981]\n",
      "Weighted sum: 0.6903\n",
      "Bias: 0.4694\n",
      "z (weighted sum + bias): 1.1597\n",
      "Final output: 1.1597\n"
     ]
    }
   ],
   "source": [
    "# Example 2: Same inputs, different activation function\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"2. Same neuron architecture but with ReLU activation\")\n",
    "neuron2 = BasicNeuron(num_inputs=2, activation_function='relu')\n",
    "print()\n",
    "neuron2.update_weights(neuron1.weights, neuron1.bias)  # Use same weights for comparison\n",
    "\n",
    "output2 = neuron2.forward(test_inputs)\n",
    "print(f\"\\nInput: {test_inputs}\")\n",
    "print(f\"Output: {output2:.4f}\")\n",
    "neuron2.get_details()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now Lets see what happens whene we give multiple inputs but random weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "3. Neuron with 4 inputs (like a more complex decision)\n",
      "Neuron created with 4 inputs\n",
      "Initial weights: [-0.93579438 -0.84967089 -0.84846857 -0.51161252]\n",
      "Initial bias: 0.11733644471822213\n",
      "Activation function: tanh\n",
      "\n",
      "Weather inputs: [0.7, 0.4, 0.2, 0.8]\n",
      "Decision output: -0.8970\n",
      "\n",
      "--- Neuron Computation Details ---\n",
      "Inputs: [0.7 0.4 0.2 0.8]\n",
      "Weights: [-0.93579438 -0.84967089 -0.84846857 -0.51161252]\n",
      "Weighted sum: -1.5739\n",
      "Bias: 0.1173\n",
      "z (weighted sum + bias): -1.4566\n",
      "Final output: -0.8970\n"
     ]
    }
   ],
   "source": [
    "# Example 3: Multiple inputs\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"3. Neuron with 4 inputs (like a more complex decision)\")\n",
    "neuron3 = BasicNeuron(num_inputs=4, activation_function='tanh')\n",
    "\n",
    "# Simulate a decision-making scenario\n",
    "# Let's say: [temperature, humidity, wind_speed, pressure]\n",
    "weather_inputs = [0.7, 0.4, 0.2, 0.8]  # Normalized values\n",
    "output3 = neuron3.forward(weather_inputs)\n",
    "print(f\"\\nWeather inputs: {weather_inputs}\")\n",
    "print(f\"Decision output: {output3:.4f}\")\n",
    "neuron3.get_details()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now lets see effects of weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neuron created with 4 inputs\n",
      "Initial weights: [-0.41190076  0.70357789 -0.28160718 -0.8136551 ]\n",
      "Initial bias: -0.8633884525838533\n",
      "Activation function: tanh\n",
      "\n",
      "Testing with inputs: [0.7, 0.4, 0.2, 0.8]\n",
      "Weights updated to: [0.5 0.5 0.5 0.5]\n",
      "Bias updated to: 0.0\n",
      "Equal weights [0.5, 0.5, 0.5, 0.5]: Output = 0.7818\n",
      "\n",
      "Weights updated to: [1.   0.5  0.25 0.1 ]\n",
      "Bias updated to: 0.0\n",
      "Different weights [1.0, 0.5, 0.25, 0.1]: Output = 0.7739\n",
      "\n",
      "Weights updated to: [0.5 1.  0.5 0.5]\n",
      "Bias updated to: 0.0\n",
      "Different weights [0.5, 1.0, 0.5, 0.5]: Output = 0.8483\n"
     ]
    }
   ],
   "source": [
    "# Example 4: Showing how weights affect output\n",
    "\n",
    "# Create a simple 2-input neuron\n",
    "neuron4 = BasicNeuron(num_inputs=4, activation_function='tanh')\n",
    "# Test different weight combinations\n",
    "output4 = neuron4.forward(weather_inputs)\n",
    "\n",
    "print(f\"\\nTesting with inputs: {weather_inputs}\")\n",
    "\n",
    "# Equal weights\n",
    "neuron4.update_weights([0.5, 0.5, 0.5, 0.5], 0.0)\n",
    "output4 = neuron4.forward(weather_inputs)\n",
    "print(f\"Equal weights [0.5, 0.5, 0.5, 0.5]: Output = {output4:.4f}\\n\")\n",
    "\n",
    "# First input more important\n",
    "neuron4.update_weights([1.0, 0.5, 0.25, 0.1], 0.0)\n",
    "output4 = neuron4.forward(weather_inputs)\n",
    "print(f\"Different weights [1.0, 0.5, 0.25, 0.1]: Output = {output4:.4f}\\n\")\n",
    "\n",
    "# Second input more important\n",
    "neuron4.update_weights([0.5, 1.0, 0.5, 0.5], 0.0)\n",
    "output4 = neuron4.forward(weather_inputs)\n",
    "print(f\"Different weights [0.5, 1.0, 0.5, 0.5]: Output = {output4:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now Lets Implement AND GATE and see what we need\n",
    "\n",
    "##### AND GATE WEIGHT DERIVATION\n",
    "\n",
    "Step 1: Understand what we need\n",
    "- When both inputs are 0: output ≈ 0\n",
    "- When one input is 1: output ≈ 0\n",
    "- When both inputs are 1: output ≈ 1\n",
    "\n",
    "Step 2: Set up the equation\n",
    "z = w1*x1 + w2*x2 + bias\n",
    "output = sigmoid(z)\n",
    "We want sigmoid(z) to be close to 0 when z is negative\n",
    "We want sigmoid(z) to be close to 1 when z is positive\n",
    "\n",
    "Step 3: Analyze each case\n",
    "Case 1: x1=0, x2=0 → z = 0*w1 + 0*w2 + bias = bias\n",
    "We want output ≈ 0, so bias should be negative (z < 0)\n",
    "\n",
    "Case 2: x1=1, x2=0 → z = 1*w1 + 0*w2 + bias = w1 + bias\n",
    "We want output ≈ 0, so w1 + bias < 0\n",
    "\n",
    "Case 3: x1=0, x2=1 → z = 0*w1 + 1*w2 + bias = w2 + bias\n",
    "We want output ≈ 0, so w2 + bias < 0\n",
    "...\n",
    "- w1 + w2 + bias > 0 → 2 + bias > 0 → bias > -2\n",
    "\n",
    "So we need: -2 < bias < -1\n",
    "Let's choose bias = -1.5 (middle of the range)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "LOGIC GATE IMPLEMENTATION WITH NEURONS\n",
      "==================================================\n",
      "\n",
      "1. AND Gate Implementation\n",
      "Neuron created with 2 inputs\n",
      "Initial weights: [ 0.79908604 -0.20282127]\n",
      "Initial bias: 0.6741794915455517\n",
      "Activation function: sigmoid\n",
      "Weights updated to: [1. 1.]\n",
      "Bias updated to: -1.5\n",
      "AND Gate Truth Table:\n",
      "  0 AND 0 = 0.1824 ≈ 0\n",
      "  0 AND 1 = 0.3775 ≈ 0\n",
      "  1 AND 0 = 0.3775 ≈ 0\n",
      "  1 AND 1 = 0.6225 ≈ 1\n"
     ]
    }
   ],
   "source": [
    "# Implementation of AND GATE\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"LOGIC GATE IMPLEMENTATION WITH NEURONS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# AND Gate\n",
    "print(\"\\n1. AND Gate Implementation\")\n",
    "and_neuron = BasicNeuron(num_inputs=2, activation_function='sigmoid')\n",
    "# Set weights and bias to implement AND logic\n",
    "and_neuron.update_weights([1.0, 1.0], -1.5)\n",
    "\n",
    "print(\"AND Gate Truth Table:\")\n",
    "for a in [0, 1]:\n",
    "    for b in [0, 1]:\n",
    "        output = and_neuron.forward([a, b])\n",
    "        print(f\"  {a} AND {b} = {output:.4f} ≈ {round(output)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now Lets Implement OR GATE and see what we need\n",
    "\n",
    "##### OR GATE WEIGHT DERIVATION\n",
    "\n",
    "\n",
    "Step 1: Understand what we need\n",
    "- When both inputs are 0: output ≈ 0\n",
    "- When any input is 1: output ≈ 1\n",
    "\n",
    "Step 2: Analyze each case\n",
    "Case 1: x1=0, x2=0 → z = bias\n",
    "We want output ≈ 0, so bias should be negative\n",
    "\n",
    "Case 2: x1=1, x2=0 → z = w1 + bias\n",
    "We want output ≈ 1, so w1 + bias > 0\n",
    "\n",
    "Case 3: x1=0, x2=1 → z = w2 + bias\n",
    "We want output ≈ 1, so w2 + bias > 0\n",
    "\n",
    "Case 4: x1=1, x2=1 → z = w1 + w2 + bias\n",
    "We want output ≈ 1, so w1 + w2 + bias > 0 (automatically satisfied)\n",
    "\n",
    "Step 3: Choose values\n",
    "Let's choose w1 = w2 = 1.0 (equal importance)\n",
    "From constraints:\n",
    "- bias < 0 (so Case 1 gives output ≈ 0)\n",
    "- w1 + bias > 0 → 1 + bias > 0 → bias > -1\n",
    "- w2 + bias > 0 → 1 + bias > 0 → bias > -1\n",
    "\n",
    "So we need: -1 < bias < 0\n",
    "Let's choose bias = -0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. OR Gate Implementation\n",
      "Neuron created with 2 inputs\n",
      "Initial weights: [ 0.90665027 -0.16672009]\n",
      "Initial bias: -0.8672119784037029\n",
      "Activation function: sigmoid\n",
      "Weights updated to: [1. 1.]\n",
      "Bias updated to: -0.5\n",
      "OR Gate Truth Table:\n",
      "  0 OR 0 = 0.3775 ≈ 0\n",
      "  0 OR 1 = 0.6225 ≈ 1\n",
      "  1 OR 0 = 0.6225 ≈ 1\n",
      "  1 OR 1 = 0.8176 ≈ 1\n"
     ]
    }
   ],
   "source": [
    "# OR Gate\n",
    "print(\"\\n2. OR Gate Implementation\")\n",
    "or_neuron = BasicNeuron(num_inputs=2, activation_function='sigmoid')\n",
    "# Set weights and bias to implement OR logic\n",
    "or_neuron.update_weights([1.0, 1.0], -0.5)\n",
    "\n",
    "print(\"OR Gate Truth Table:\")\n",
    "for a in [0, 1]:\n",
    "    for b in [0, 1]:\n",
    "        output = or_neuron.forward([a, b])\n",
    "        print(f\"  {a} OR {b} = {output:.4f} ≈ {round(output)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. OR Gate Implementation\n",
      "Neuron created with 2 inputs\n",
      "Initial weights: [-0.99308742 -0.06922159]\n",
      "Initial bias: -0.18289538879693157\n",
      "Activation function: sigmoid\n",
      "Weights updated to: [-1. -1.]\n",
      "Bias updated to: 0.5\n",
      "OR Gate Truth Table:\n",
      "  0 OR 0 = 0.6225 ≈ 1\n",
      "  0 OR 1 = 0.3775 ≈ 0\n",
      "  1 OR 0 = 0.3775 ≈ 0\n",
      "  1 OR 1 = 0.1824 ≈ 0\n"
     ]
    }
   ],
   "source": [
    "# NOR Gate\n",
    "print(\"\\n2. OR Gate Implementation\")\n",
    "or_neuron = BasicNeuron(num_inputs=2, activation_function='sigmoid')\n",
    "# Set weights and bias to implement OR logic\n",
    "or_neuron.update_weights([-1.0, -1.0], 0.5)\n",
    "\n",
    "print(\"OR Gate Truth Table:\")\n",
    "for a in [0, 1]:\n",
    "    for b in [0, 1]:\n",
    "        output = or_neuron.forward([a, b])\n",
    "        print(f\"  {a} OR {b} = {output:.4f} ≈ {round(output)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "2. OR Gate Implementation\n",
      "Neuron created with 2 inputs\n",
      "Initial weights: [-0.74366582  0.20334773]\n",
      "Initial bias: 0.7750696620282955\n",
      "Activation function: sigmoid\n",
      "Weights updated to: [-0.3 -0.3]\n",
      "Bias updated to: 0.5\n",
      "OR Gate Truth Table:\n",
      "  0 OR 0 = 0.6225 ≈ 1\n",
      "  0 OR 1 = 0.5498 ≈ 1\n",
      "  1 OR 0 = 0.5498 ≈ 1\n",
      "  1 OR 1 = 0.4750 ≈ 0\n"
     ]
    }
   ],
   "source": [
    "# NAND Gate\n",
    "print(\"\\n2. OR Gate Implementation\")\n",
    "or_neuron = BasicNeuron(num_inputs=2, activation_function='sigmoid')\n",
    "# Set weights and bias to implement OR logic\n",
    "or_neuron.update_weights([-0.3, -0.3], 0.5)\n",
    "\n",
    "print(\"OR Gate Truth Table:\")\n",
    "for a in [0, 1]:\n",
    "    for b in [0, 1]:\n",
    "        output = or_neuron.forward([a, b])\n",
    "        print(f\"  {a} OR {b} = {output:.4f} ≈ {round(output)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### On why XOR Gate can't be made using the same method\n",
    "XOR Gate Truth Table\n",
    "0 XOR 0 = 0\n",
    "0 XOR 1 = 1\n",
    "1 XOR 0 = 1\n",
    "1 XOR 1 = 0\n",
    "\n",
    "For 0 XOR 0 = 0\n",
    "    0 * weight[0] + 0 * weight[1] + bias <= 0 => bias<=0\n",
    "For 0 XOR 1 = 1\n",
    "    0 * weight[0] + 1 * weight[1] + bias >= 0 => weight[1] + bias >=0 => {assuming bias<=0} weight[1]>=-bias => weight[1]>=0\n",
    "For 1 XOR 0 = 1\n",
    "    1 * weight[0] + 0 * weight[1] + bias >= 0 => weight[0] + bias >=0 => {assuming bias<=0} weight[0]>=-bias => weight[0]>=0\n",
    "\n",
    "This implies that \n",
    "    weight[0]=0, when bias=0\n",
    "    weight[0]>0, when bias<0\n",
    "    {Similary for weight[1]}    \n",
    "\n",
    "For 1 XOR 1 = 1\n",
    "    =>  weight[0] + weight[1] >= 0\n",
    "        weight[0] + weight[1] = 0, when bias = 0\n",
    "        weight[0] + weight[1] > 2*(-bias), when bias<0\n",
    "        weight[0] + weight[1] > 0, when bias<0\n",
    "\n",
    "        But this contrasts with what we really want\n",
    "        1 XOR 1 = 0\n",
    "        1 * weight[0] + 1 * weight[1] + bias <= 0 => weight[0] + weight[1] + bias <= 0\n",
    "\n",
    " \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
