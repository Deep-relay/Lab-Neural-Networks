{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6aec8f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f2b2bdeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicNeuron:\n",
    "    \"\"\"\n",
    "    A basic artificial neuron implementation that mimics biological neurons.\n",
    "\n",
    "    The neuron receives inputs, applies weights, adds bias, and produces an output\n",
    "    through an activation function - just like neurons in our brain!\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, num_inputs, activation_function='sigmoid'):\n",
    "        \"\"\"\n",
    "        Initialize the neuron with random weights and bias.\n",
    "\n",
    "        Args:\n",
    "            num_inputs: Number of input connections to this neuron\n",
    "            activation_function: Type of activation function ('sigmoid', 'relu', 'tanh', 'linear')\n",
    "        \"\"\"\n",
    "        # Initialize weights randomly between -1 and 1\n",
    "        # Each input gets its own weight - this determines how important each input is\n",
    "        self.weights = np.random.uniform(-1, 1, num_inputs)\n",
    "\n",
    "        # Initialize bias - this shifts the activation function left or right\n",
    "        # Bias helps the neuron fire even when inputs are small\n",
    "        self.bias = np.random.uniform(-1, 1)\n",
    "\n",
    "        # Store the activation function type\n",
    "        if activation_function not in ['sigmoid', 'relu', 'tanh', 'linear']:\n",
    "            raise ValueError(f\"Unsupported activation function: {activation_function}\")\n",
    "        self.activation_function = activation_function\n",
    "\n",
    "        # Store the number of inputs for validation\n",
    "        self.num_inputs = num_inputs\n",
    "\n",
    "        # print(f\"Neuron created with {num_inputs} inputs\")\n",
    "        # print(f\"Initial weights: {self.weights}\")\n",
    "        # print(f\"Initial bias: {self.bias}\")\n",
    "        # print(f\"Activation function: {activation_function}\")\n",
    "\n",
    "    def sigmoid(self, x):\n",
    "        \"\"\"\n",
    "        Sigmoid activation function: f(x) = 1 / (1 + e^(-x))\n",
    "\n",
    "        - Outputs values between 0 and 1\n",
    "        - Smooth, differentiable curve\n",
    "        - Good for binary classification problems\n",
    "        \"\"\"\n",
    "        # Clip x to prevent overflow in exponential\n",
    "        x = np.clip(x, -500, 500)\n",
    "        return 1 / (1 + np.exp(-x))\n",
    "\n",
    "    def sigmoid_derivative(self, x):\n",
    "        \"\"\"Derivative of the sigmoid function.\"\"\"\n",
    "        # Derivative of sigmoid(x) is sigmoid(x) * (1 - sigmoid(x))\n",
    "        # We can use the output of the sigmoid function itself for this\n",
    "        s = self.sigmoid(x)\n",
    "        return s * (1 - s)\n",
    "\n",
    "    def relu(self, x):\n",
    "        \"\"\"ReLU activation function: f(x) = max(0, x)\"\"\"\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def relu_derivative(self, x):\n",
    "        \"\"\"Derivative of the ReLU function.\"\"\"\n",
    "        return np.where(x > 0, 1, 0)\n",
    "\n",
    "    def tanh(self, x):\n",
    "        \"\"\"Tanh activation function: f(x) = tanh(x)\"\"\"\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def tanh_derivative(self, x):\n",
    "        \"\"\"Derivative of the Tanh function.\"\"\"\n",
    "        # Derivative of tanh(x) is 1 - tanh(x)^2\n",
    "        return 1 - np.tanh(x)**2\n",
    "\n",
    "    def linear(self, x):\n",
    "        \"\"\"Linear activation function: f(x) = x\"\"\"\n",
    "        return x\n",
    "\n",
    "    def linear_derivative(self, x):\n",
    "        \"\"\"Derivative of the Linear function.\"\"\"\n",
    "        return 1\n",
    "\n",
    "    def get_activation_derivative(self, z):\n",
    "        \"\"\"Returns the derivative of the chosen activation function for a given net input z.\"\"\"\n",
    "        if self.activation_function == 'sigmoid':\n",
    "            return self.sigmoid_derivative(z)\n",
    "        elif self.activation_function == 'relu':\n",
    "            return self.relu_derivative(z)\n",
    "        elif self.activation_function == 'tanh':\n",
    "            return self.tanh_derivative(z)\n",
    "        elif self.activation_function == 'linear':\n",
    "            return self.linear_derivative(z)\n",
    "        else:\n",
    "            raise ValueError(f\"Derivative not implemented for {self.activation_function}\")\n",
    "\n",
    "\n",
    "    def forward(self, inputs):\n",
    "\n",
    "        # Convert inputs to numpy array for easier computation\n",
    "        inputs = np.array(inputs)\n",
    "\n",
    "        # Validate input size\n",
    "        if len(inputs) != self.num_inputs:\n",
    "            raise ValueError(f\"Expected {self.num_inputs} inputs, got {len(inputs)}\")\n",
    "\n",
    "        # Step 1 & 2: Weighted sum (dot product of inputs and weights)\n",
    "        # This is like: w1*x1 + w2*x2 + w3*x3 + ... + wn*xn\n",
    "        weighted_sum = np.dot(inputs, self.weights)\n",
    "\n",
    "        # Step 3: Add bias\n",
    "        # Bias allows the neuron to fire even when inputs are zero\n",
    "        z = weighted_sum + self.bias\n",
    "\n",
    "        # Step 4: Apply activation function\n",
    "        if self.activation_function == 'sigmoid':\n",
    "            output = self.sigmoid(z)\n",
    "        elif self.activation_function == 'relu':\n",
    "            output = self.relu(z)\n",
    "        elif self.activation_function == 'tanh':\n",
    "            output = self.tanh(z)\n",
    "        elif self.activation_function == 'linear':\n",
    "            output = self.linear(z)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown activation function: {self.activation_function}\")\n",
    "\n",
    "        # Store intermediate values for educational purposes\n",
    "        self.last_inputs = inputs\n",
    "        self.last_weighted_sum = weighted_sum\n",
    "        self.last_z = z # Store z (net input before activation) for derivative calculation\n",
    "        self.last_output = output\n",
    "\n",
    "        return output\n",
    "\n",
    "    def update_weights(self, new_weights, new_bias=None):\n",
    "        if len(new_weights) != self.num_inputs:\n",
    "            raise ValueError(f\"Expected {self.num_inputs} weights, got {len(new_weights)}\")\n",
    "\n",
    "        self.weights = np.array(new_weights)\n",
    "\n",
    "        if new_bias is not None:\n",
    "            self.bias = new_bias\n",
    "\n",
    "        # print(f\"Weights updated to: {self.weights}\")\n",
    "        # print(f\"Bias updated to: {self.bias}\")\n",
    "\n",
    "    def get_details(self, verbose=True):\n",
    "        if hasattr(self, 'last_inputs'):\n",
    "            if verbose:\n",
    "                print(\"\\n--- Neuron Computation Details ---\")\n",
    "                print(f\"Inputs: {self.last_inputs}\")\n",
    "                print(f\"Weights: {self.weights}\")\n",
    "                print(f\"Weighted sum: {self.last_weighted_sum:.4f}\")\n",
    "                print(f\"Bias: {self.bias:.4f}\")\n",
    "                print(f\"z (weighted sum + bias): {self.last_z:.4f}\")\n",
    "                print(f\"Final output: {self.last_output:.4f}\")\n",
    "        else:\n",
    "            if verbose:\n",
    "                print(\"No computation has been performed yet!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35eca926",
   "metadata": {},
   "outputs": [],
   "source": [
    "#correlation\n",
    "\n",
    "def correlation_learning(inputs, outputs, learning_rate=1.0):\n",
    "    \"\"\"\n",
    "    Implements the Correlation Learning Law similar to Hebbian learning.\n",
    "    - inputs: shape (num_patterns, num_inputs)\n",
    "    - outputs: shape (num_patterns,) or (num_patterns, num_outputs)\n",
    "    \"\"\"\n",
    "    num_inputs = inputs.shape[1]\n",
    "    neuron = BasicNeuron(num_inputs, activation_function='linear')\n",
    "    # Initialize weights to zeros for simplicity\n",
    "    neuron.update_weights(np.zeros(num_inputs), 0.0)\n",
    "\n",
    "    num_patterns = inputs.shape[0]\n",
    "\n",
    "    print(f\"\\n--- Correlation Learning ---\")\n",
    "    print(f\"Initial Weights: {neuron.weights}, Initial Bias: {neuron.bias}\")\n",
    "\n",
    "    for i in range(num_patterns):\n",
    "        x = inputs[i]\n",
    "        y_desired = outputs[i]\n",
    "\n",
    "        # Correlation learning rule:\n",
    "        # delta_w = learning_rate * (x * y_desired)\n",
    "        # similar to Hebbian, but emphasizes correlation between x and y\n",
    "        delta_weights = learning_rate * np.outer(x, [y_desired]).flatten()\n",
    "\n",
    "        new_weights = neuron.weights + delta_weights\n",
    "        new_bias = neuron.bias  # correlation law typically ignores bias update\n",
    "\n",
    "        neuron.update_weights(new_weights, new_bias)\n",
    "\n",
    "        print(f\"Pattern {i+1}: Input={x}, Desired Output={y_desired}\")\n",
    "        print(f\"  Delta Weights: {delta_weights}\")\n",
    "        print(f\"  Updated Weights: {neuron.weights}, Updated Bias: {neuron.bias}\")\n",
    "\n",
    "    print(f\"Final Weights (Correlation): {neuron.weights}, Final Bias: {neuron.bias}\")\n",
    "    return neuron.weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "be9aef3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Correlation Learning ---\n",
      "Initial Weights: [0. 0.], Initial Bias: 0.0\n",
      "Pattern 1: Input=[1 0], Desired Output=0\n",
      "  Delta Weights: [0. 0.]\n",
      "  Updated Weights: [0. 0.], Updated Bias: 0.0\n",
      "Pattern 2: Input=[0 1], Desired Output=0\n",
      "  Delta Weights: [0. 0.]\n",
      "  Updated Weights: [0. 0.], Updated Bias: 0.0\n",
      "Pattern 3: Input=[1 1], Desired Output=1\n",
      "  Delta Weights: [1. 1.]\n",
      "  Updated Weights: [1. 1.], Updated Bias: 0.0\n",
      "Pattern 4: Input=[0 0], Desired Output=0\n",
      "  Delta Weights: [0. 0.]\n",
      "  Updated Weights: [1. 1.], Updated Bias: 0.0\n",
      "Final Weights (Correlation): [1. 1.], Final Bias: 0.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([1., 1.])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Correlation_inputs = np.array([\n",
    "    [1, 0],\n",
    "    [0, 1],\n",
    "    [1, 1],\n",
    "    [0, 0]\n",
    "])\n",
    "Correlation_outputs = np.array([\n",
    "    0,  \n",
    "    0, \n",
    "    1,  \n",
    "    0   \n",
    "])\n",
    "correlation_learning(Correlation_inputs, Correlation_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74fbcff5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#outstar\n",
    "\n",
    "def outstar_learning(inputs, outputs, learning_rate=0.1, max_epochs=100, tolerance=1e-6):\n",
    "    \"\"\"\n",
    "    Implements Outstar Learning with convergence.\n",
    "    - inputs: shape (num_patterns, num_inputs)\n",
    "    - outputs: shape (num_patterns, num_outputs) or (num_patterns,)\n",
    "    - learning_rate: step size for updates\n",
    "    - max_epochs: maximum number of epochs\n",
    "    - tolerance: convergence threshold\n",
    "    \"\"\"\n",
    "    num_inputs = inputs.shape[1]\n",
    "    neuron = BasicNeuron(num_inputs, activation_function='linear')\n",
    "    \n",
    "    # Initialize weights to zeros\n",
    "    neuron.update_weights(np.zeros(num_inputs), 0.0)\n",
    "    \n",
    "    print(\"\\n--- Outstar Learning with Convergence ---\")\n",
    "    print(f\"Initial Weights: {neuron.weights}\")\n",
    "    \n",
    "    for epoch in range(max_epochs):\n",
    "        total_change = 0.0\n",
    "        \n",
    "        # Outstar rule is teacher-driven → we use outputs directly\n",
    "        for i in range(inputs.shape[0]):\n",
    "            y_desired = outputs[i]\n",
    "            \n",
    "            # Update rule: w <- w + η (y_desired - w)\n",
    "            delta_w = learning_rate * (y_desired - neuron.weights)\n",
    "            new_weights = neuron.weights + delta_w\n",
    "            \n",
    "            total_change += np.linalg.norm(delta_w)\n",
    "            \n",
    "            neuron.update_weights(new_weights, neuron.bias)  # bias unchanged\n",
    "        \n",
    "        # print(f\"Epoch {epoch+1}: Weights={neuron.weights}, Change={total_change:.6f}\")\n",
    "        \n",
    "        # Convergence check\n",
    "        if total_change < tolerance:\n",
    "            print(f\"Converged after {epoch+1} epochs.\")\n",
    "            break\n",
    "    \n",
    "    print(f\"Final Weights: {neuron.weights}\")\n",
    "    return neuron.weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "190a8e42",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Outstar Learning with Convergence ---\n",
      "Initial Weights: [0. 0.]\n",
      "Final Weights: [0.7092178 0.7092178]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.7092178, 0.7092178])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Outstar_inputs = np.array([\n",
    "    [1, 0],\n",
    "    [0, 1],\n",
    "    [1, 1],\n",
    "    [0, 0]\n",
    "])\n",
    "Outstar_outputs = np.array([\n",
    "    1,  \n",
    "    1, \n",
    "    1,  \n",
    "    0   \n",
    "])\n",
    "outstar_learning(Outstar_inputs, Outstar_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aa93515",
   "metadata": {},
   "outputs": [],
   "source": [
    "#competitive\n",
    "\n",
    "def instar_learning(inputs, num_neurons=2, learning_rate=0.5, epochs=10):\n",
    "    \"\"\"\n",
    "    Implements the Instar Learning Law (Grossberg) with Winner-Take-All.\n",
    "    Used for competitive learning / clustering. Neurons compete to respond to an input pattern.\n",
    "    The winner's weights are moved closer to the input pattern.\n",
    "    Update rule for winning neuron j: Δw_j = η * (x - w_j)\n",
    "    \"\"\"\n",
    "    num_inputs = inputs.shape[1]\n",
    "    # Create a layer of neurons\n",
    "    neurons = [BasicNeuron(num_inputs, activation_function='linear') for _ in range(num_neurons)]\n",
    "\n",
    "    print(f\"\\n--- Instar (Winner-Take-All) Learning Law ---\")\n",
    "    for i, n in enumerate(neurons):\n",
    "        print(f\"Initial Weights for Neuron {i+1}: {n.weights}\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\n--- Epoch {epoch+1} ---\")\n",
    "        for x in inputs:\n",
    "            # 1. Competition: Find the winning neuron (closest weight vector)\n",
    "            # The one with the highest dot product (net input) wins.\n",
    "            net_inputs = [np.dot(n.weights, x) for n in neurons]\n",
    "            winner_index = np.argmax(net_inputs)\n",
    "            winner_neuron = neurons[winner_index]\n",
    "\n",
    "            # 2. Update winner's weights\n",
    "            delta_weights = learning_rate * (x - winner_neuron.weights)\n",
    "            new_weights = winner_neuron.weights + delta_weights\n",
    "            winner_neuron.update_weights(new_weights)\n",
    "            \n",
    "            print(f\"Input: {x}, Winner: Neuron {winner_index+1}, Updated Weights: {winner_neuron.weights}\")\n",
    "\n",
    "    print(\"\\nFinal Weights (Instar):\")\n",
    "    for i, n in enumerate(neurons):\n",
    "        print(f\"Neuron {i+1}: {n.weights}\")\n",
    "    return [n.weights for n in neurons]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3116551c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Instar (Winner-Take-All) Learning Law ---\n",
      "Initial Weights for Neuron 1: [ 0.42563814 -0.02468607]\n",
      "Initial Weights for Neuron 2: [ 0.92569105 -0.9065473 ]\n",
      "\n",
      "--- Epoch 1 ---\n",
      "Input: [1 0], Winner: Neuron 2, Updated Weights: [ 0.96284553 -0.45327365]\n",
      "Input: [0 1], Winner: Neuron 1, Updated Weights: [0.21281907 0.48765696]\n",
      "Input: [1 1], Winner: Neuron 1, Updated Weights: [0.60640953 0.74382848]\n",
      "Input: [0 0], Winner: Neuron 1, Updated Weights: [0.30320477 0.37191424]\n",
      "\n",
      "--- Epoch 2 ---\n",
      "Input: [1 0], Winner: Neuron 2, Updated Weights: [ 0.98142276 -0.22663682]\n",
      "Input: [0 1], Winner: Neuron 1, Updated Weights: [0.15160238 0.68595712]\n",
      "Input: [1 1], Winner: Neuron 1, Updated Weights: [0.57580119 0.84297856]\n",
      "Input: [0 0], Winner: Neuron 1, Updated Weights: [0.2879006  0.42148928]\n",
      "\n",
      "--- Epoch 3 ---\n",
      "Input: [1 0], Winner: Neuron 2, Updated Weights: [ 0.99071138 -0.11331841]\n",
      "Input: [0 1], Winner: Neuron 1, Updated Weights: [0.1439503  0.71074464]\n",
      "Input: [1 1], Winner: Neuron 2, Updated Weights: [0.99535569 0.44334079]\n",
      "Input: [0 0], Winner: Neuron 1, Updated Weights: [0.07197515 0.35537232]\n",
      "\n",
      "--- Epoch 4 ---\n",
      "Input: [1 0], Winner: Neuron 2, Updated Weights: [0.99767785 0.2216704 ]\n",
      "Input: [0 1], Winner: Neuron 1, Updated Weights: [0.03598757 0.67768616]\n",
      "Input: [1 1], Winner: Neuron 2, Updated Weights: [0.99883892 0.6108352 ]\n",
      "Input: [0 0], Winner: Neuron 1, Updated Weights: [0.01799379 0.33884308]\n",
      "\n",
      "--- Epoch 5 ---\n",
      "Input: [1 0], Winner: Neuron 2, Updated Weights: [0.99941946 0.3054176 ]\n",
      "Input: [0 1], Winner: Neuron 1, Updated Weights: [0.00899689 0.66942154]\n",
      "Input: [1 1], Winner: Neuron 2, Updated Weights: [0.99970973 0.6527088 ]\n",
      "Input: [0 0], Winner: Neuron 1, Updated Weights: [0.00449845 0.33471077]\n",
      "\n",
      "--- Epoch 6 ---\n",
      "Input: [1 0], Winner: Neuron 2, Updated Weights: [0.99985487 0.3263544 ]\n",
      "Input: [0 1], Winner: Neuron 1, Updated Weights: [0.00224922 0.66735539]\n",
      "Input: [1 1], Winner: Neuron 2, Updated Weights: [0.99992743 0.6631772 ]\n",
      "Input: [0 0], Winner: Neuron 1, Updated Weights: [0.00112461 0.33367769]\n",
      "\n",
      "--- Epoch 7 ---\n",
      "Input: [1 0], Winner: Neuron 2, Updated Weights: [0.99996372 0.3315886 ]\n",
      "Input: [0 1], Winner: Neuron 1, Updated Weights: [5.62305851e-04 6.66838846e-01]\n",
      "Input: [1 1], Winner: Neuron 2, Updated Weights: [0.99998186 0.6657943 ]\n",
      "Input: [0 0], Winner: Neuron 1, Updated Weights: [2.81152926e-04 3.33419423e-01]\n",
      "\n",
      "--- Epoch 8 ---\n",
      "Input: [1 0], Winner: Neuron 2, Updated Weights: [0.99999093 0.33289715]\n",
      "Input: [0 1], Winner: Neuron 1, Updated Weights: [1.40576463e-04 6.66709712e-01]\n",
      "Input: [1 1], Winner: Neuron 2, Updated Weights: [0.99999546 0.66644857]\n",
      "Input: [0 0], Winner: Neuron 1, Updated Weights: [7.02882314e-05 3.33354856e-01]\n",
      "\n",
      "--- Epoch 9 ---\n",
      "Input: [1 0], Winner: Neuron 2, Updated Weights: [0.99999773 0.33322429]\n",
      "Input: [0 1], Winner: Neuron 1, Updated Weights: [3.51441157e-05 6.66677428e-01]\n",
      "Input: [1 1], Winner: Neuron 2, Updated Weights: [0.99999887 0.66661214]\n",
      "Input: [0 0], Winner: Neuron 1, Updated Weights: [1.75720579e-05 3.33338714e-01]\n",
      "\n",
      "--- Epoch 10 ---\n",
      "Input: [1 0], Winner: Neuron 2, Updated Weights: [0.99999943 0.33330607]\n",
      "Input: [0 1], Winner: Neuron 1, Updated Weights: [8.78602893e-06 6.66669357e-01]\n",
      "Input: [1 1], Winner: Neuron 2, Updated Weights: [0.99999972 0.66665304]\n",
      "Input: [0 0], Winner: Neuron 1, Updated Weights: [4.39301446e-06 3.33334678e-01]\n",
      "\n",
      "Final Weights (Instar):\n",
      "Neuron 1: [4.39301446e-06 3.33334678e-01]\n",
      "Neuron 2: [0.99999972 0.66665304]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[array([4.39301446e-06, 3.33334678e-01]), array([0.99999972, 0.66665304])]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "competitive_inputs = np.array([\n",
    "    [1, 0],\n",
    "    [0, 1],\n",
    "    [1, 1],\n",
    "    [0, 0]\n",
    "])\n",
    "instar_learning(competitive_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14a450f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#reinforcement\n",
    "\n",
    "def reinforcement_learning(inputs, rewards, learning_rate=1.0):\n",
    "    \"\"\"\n",
    "    Implements a simple Reinforcement Learning rule.\n",
    "    - inputs: shape (num_patterns, num_inputs)\n",
    "    - rewards: shape (num_patterns,) reinforcement signals (+1 for reward, -1 for punishment, 0 for neutral)\n",
    "    \"\"\"\n",
    "    num_inputs = inputs.shape[1]\n",
    "    neuron = BasicNeuron(num_inputs, activation_function='linear')\n",
    "    # Initialize weights to zeros for simplicity\n",
    "    neuron.update_weights(np.zeros(num_inputs), 0.0)\n",
    "\n",
    "    num_patterns = inputs.shape[0]\n",
    "\n",
    "    print(f\"\\n--- Reinforcement Learning ---\")\n",
    "    print(f\"Initial Weights: {neuron.weights}, Initial Bias: {neuron.bias}\")\n",
    "\n",
    "    for i in range(num_patterns):\n",
    "        x = inputs[i]\n",
    "        r = rewards[i]\n",
    "\n",
    "        # Current neuron output\n",
    "        y = neuron.forward(x)\n",
    "\n",
    "        delta_weights = learning_rate * r * x\n",
    "\n",
    "        new_weights = neuron.weights + delta_weights\n",
    "        new_bias = neuron.bias  # bias is often left unchanged here\n",
    "\n",
    "        neuron.update_weights(new_weights, new_bias)\n",
    "\n",
    "        print(f\"Pattern {i+1}: Input={x}, Reward={r}, Output={y}\")\n",
    "        print(f\"  Delta Weights: {delta_weights}\")\n",
    "        print(f\"  Updated Weights: {neuron.weights}, Updated Bias: {neuron.bias}\")\n",
    "\n",
    "    print(f\"Final Weights: {neuron.weights}, Final Bias: {neuron.bias}\")\n",
    "\n",
    "    # Test with last input\n",
    "    test_output = neuron.forward(inputs[-1])\n",
    "    print(f\"Test output for last pattern: {test_output}\")\n",
    "\n",
    "    return neuron.weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2976177",
   "metadata": {},
   "outputs": [],
   "source": [
    "#boltzmann\n",
    "\n",
    "def boltzmann_learning(inputs, outputs, learning_rate=0.1, epochs=10):\n",
    "    \"\"\"\n",
    "    Implements a simplified Boltzmann Learning Law using the BasicNeuron class.\n",
    "    \n",
    "    Args:\n",
    "        inputs: shape (num_patterns, num_inputs)\n",
    "        outputs: shape (num_patterns,) - desired target outputs (clamped phase)\n",
    "        learning_rate: learning rate (η)\n",
    "        epochs: number of training iterations\n",
    "\n",
    "    Returns:\n",
    "        Final learned weights\n",
    "    \"\"\"\n",
    "    num_inputs = inputs.shape[1]\n",
    "    neuron = BasicNeuron(num_inputs, activation_function='sigmoid')  # sigmoid is typical for Boltzmann machines\n",
    "    \n",
    "    # Initialize weights small\n",
    "    neuron.update_weights(np.random.uniform(-0.1, 0.1, num_inputs), 0.0)\n",
    "\n",
    "    print(\"\\n--- Boltzmann Learning ---\")\n",
    "    print(f\"Initial Weights: {neuron.weights}, Initial Bias: {neuron.bias}\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}:\")\n",
    "        \n",
    "        for i in range(inputs.shape[0]):\n",
    "            x = inputs[i]\n",
    "            y_clamped = outputs[i]  # \"data\" expectation (clamped phase)\n",
    "\n",
    "            # --- Clamped phase (desired output) ---\n",
    "            clamped_corr = np.outer(x, [y_clamped]).flatten()\n",
    "\n",
    "            # --- Free phase (model output) ---\n",
    "            y_free = neuron.forward(x)  # model’s own prediction\n",
    "            free_corr = np.outer(x, [y_free]).flatten()\n",
    "\n",
    "            # --- Weight update rule (Boltzmann Learning) ---\n",
    "            delta_weights = learning_rate * (clamped_corr - free_corr)\n",
    "\n",
    "            new_weights = neuron.weights + delta_weights\n",
    "            neuron.update_weights(new_weights, neuron.bias)\n",
    "\n",
    "            print(f\"  Pattern {i+1}: Input={x}, Desired={y_clamped}, Free={y_free:.4f}\")\n",
    "            print(f\"    Clamped Corr: {clamped_corr}\")\n",
    "            print(f\"    Free Corr:    {free_corr}\")\n",
    "            print(f\"    Delta W:      {delta_weights}\")\n",
    "            print(f\"    Updated W:    {neuron.weights}\")\n",
    "\n",
    "    print(f\"\\nFinal Weights (Boltzmann): {neuron.weights}, Final Bias: {neuron.bias}\")\n",
    "    return neuron.weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86260775",
   "metadata": {},
   "outputs": [],
   "source": [
    "#widrow hoff lms\n",
    "\n",
    "def widrow_hoff_learning(inputs, outputs, learning_rate=0.1, epochs=10):\n",
    "    \"\"\"\n",
    "    Implements the Widrow-Hoff (LMS/Delta Rule) learning law.\n",
    "    \n",
    "    Args:\n",
    "        inputs: shape (num_patterns, num_inputs)\n",
    "        outputs: shape (num_patterns,) - desired outputs\n",
    "        learning_rate: learning rate (η)\n",
    "        epochs: number of training iterations\n",
    "    \n",
    "    Returns:\n",
    "        Final learned weights and bias\n",
    "    \"\"\"\n",
    "    num_inputs = inputs.shape[1]\n",
    "    neuron = BasicNeuron(num_inputs, activation_function='linear')  # LMS is usually linear\n",
    "    \n",
    "    # Initialize weights small\n",
    "    neuron.update_weights(np.random.uniform(-0.1, 0.1, num_inputs), 0.0)\n",
    "\n",
    "    print(\"\\n--- Widrow-Hoff LMS Learning ---\")\n",
    "    print(f\"Initial Weights: {neuron.weights}, Initial Bias: {neuron.bias}\")\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        print(f\"\\nEpoch {epoch+1}:\")\n",
    "        \n",
    "        for i in range(inputs.shape[0]):\n",
    "            x = inputs[i]\n",
    "            y_desired = outputs[i]\n",
    "\n",
    "            # Forward pass (actual output)\n",
    "            y_actual = neuron.forward(x)\n",
    "\n",
    "            # Compute error\n",
    "            error = y_desired - y_actual\n",
    "\n",
    "            # Weight update (Delta Rule)\n",
    "            delta_weights = learning_rate * error * x\n",
    "            delta_bias = learning_rate * error\n",
    "\n",
    "            new_weights = neuron.weights + delta_weights\n",
    "            new_bias = neuron.bias + delta_bias\n",
    "\n",
    "            neuron.update_weights(new_weights, new_bias)\n",
    "\n",
    "            print(f\"  Pattern {i+1}: Input={x}, Desired={y_desired}, Actual={y_actual:.4f}\")\n",
    "            print(f\"    Error: {error:.4f}\")\n",
    "            print(f\"    ΔW: {delta_weights}, ΔB: {delta_bias:.4f}\")\n",
    "            print(f\"    Updated W: {neuron.weights}, Updated B: {neuron.bias:.4f}\")\n",
    "\n",
    "    print(f\"\\nFinal Weights (Widrow-Hoff): {neuron.weights}, Final Bias: {neuron.bias}\")\n",
    "    return neuron.weights, neuron.bias\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "(Lab) Neural Networks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
